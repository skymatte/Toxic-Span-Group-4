{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYV12Hv-k9lt"
      },
      "source": [
        "# Lint as: python3\n",
        "\"\"\"Example tagging for Toxic Spans based on Spacy.\n",
        "Requires:\n",
        "  pip install spacy sklearn\n",
        "Install models:\n",
        "  python -m spacy download en_core_web_sm\n",
        "\"\"\"\n",
        "\n",
        "import ast\n",
        "import csv\n",
        "import random\n",
        "import statistics\n",
        "import sys\n",
        "import itertools\n",
        "import string\n",
        "import sklearn\n",
        "import spacy\n",
        "\n",
        "sys.path.append('../evaluation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIwKMuBxy6vF"
      },
      "source": [
        "accu=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1vt_hYTJYfq"
      },
      "source": [
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set) + len(gold_set)\n",
        "    return float(nom)/float(denom)\n",
        "\n",
        "\n",
        "def evaluate(pred, gold):\n",
        "    \"\"\"\n",
        "    Based on https://github.com/felipebravom/EmoInt/blob/master/codalab/scoring_program/evaluation.py\n",
        "    :param pred: file with predictions\n",
        "    :param gold: file with ground truth\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # read the predictions\n",
        "    pred_lines = pred.readlines()\n",
        "    # read the ground truth\n",
        "    gold_lines = gold.readlines()\n",
        "\n",
        "    # only when the same number of lines exists\n",
        "    if (len(pred_lines) == len(gold_lines)):\n",
        "        data_dic = {}\n",
        "        for n, line in enumerate(gold_lines):\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                data_dic[int(parts[0])] = [literal_eval(parts[1])]\n",
        "            else:\n",
        "                raise ValueError('Format problem for gold line %d.', n)\n",
        "\n",
        "        for n, line in enumerate(pred_lines):\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                if int(parts[0]) in data_dic:\n",
        "                    try:\n",
        "                        data_dic[int(parts[0])].append(literal_eval(parts[1]))\n",
        "                    except ValueError:\n",
        "                        # Invalid predictions are replaced by a default value\n",
        "                        data_dic[int(parts[0])].append([])\n",
        "                else:\n",
        "                    raise ValueError('Invalid text id for pred line %d.', n)\n",
        "            else:\n",
        "                raise ValueError('Format problem for pred line %d.', n)\n",
        "\n",
        "        # lists storing gold and prediction scores\n",
        "        scores = []\n",
        "        for id in data_dic:\n",
        "            if len(data_dic[id]) == 2:\n",
        "                gold_spans = data_dic[id][0]\n",
        "                pred_spans = data_dic[id][1]\n",
        "                scores.append(f1(pred_spans, gold_spans))\n",
        "            else:\n",
        "                sys.exit('Repeated id in test data.')\n",
        "\n",
        "        return (np.mean(scores), sem(scores))\n",
        "\n",
        "    else:\n",
        "        sys.exit('Predictions and gold data have different number of lines.')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-GNvy2aJNoS"
      },
      "source": [
        "def _contiguous_ranges(span_list):\n",
        "    \"\"\"Extracts continguous runs [1, 2, 3, 5, 6, 7] -> [(1,3), (5,7)].\"\"\"\n",
        "    output = []\n",
        "    for _, span in itertools.groupby(\n",
        "        enumerate(span_list), lambda p: p[1] - p[0]):\n",
        "        span = list(span)\n",
        "        output.append((span[0][1], span[-1][1]))\n",
        "    return output\n",
        "\n",
        "SPECIAL_CHARACTERS = string.whitespace\n",
        "def fix_spans(spans, text, special_characters=SPECIAL_CHARACTERS):\n",
        "    \"\"\"Applies minor edits to trim spans and remove singletons.\"\"\"\n",
        "    cleaned = []\n",
        "    for begin, end in _contiguous_ranges(spans):\n",
        "        if end>=len(text):\n",
        "            # if begin>=len(text):\n",
        "            # else :\n",
        "            continue\n",
        "        while text[begin] in special_characters and begin < end:\n",
        "            begin += 1\n",
        "        while text[end] in special_characters and begin < end:\n",
        "            end -= 1\n",
        "        if end - begin > 1:\n",
        "            cleaned.extend(range(begin, end + 1))\n",
        "    return cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD6yWkiNk2vG"
      },
      "source": [
        "def spans_to_ents(doc, spans, label):\n",
        "  \"\"\"Converts span indicies into spacy entity labels.\"\"\"\n",
        "  started = False\n",
        "  left, right, ents = 0, 0, []\n",
        "  for x in doc:\n",
        "    if x.pos_ == 'SPACE':\n",
        "      continue\n",
        "    if spans.intersection(set(range(x.idx, x.idx + len(x.text)))):\n",
        "      if not started:\n",
        "        left, started = x.idx, True\n",
        "      right = x.idx + len(x.text)\n",
        "    elif started:\n",
        "      ents.append((left, right, label))\n",
        "      started = False\n",
        "  if started:\n",
        "    ents.append((left, right, label))\n",
        "  return ents\n",
        "\n",
        "\n",
        "def read_datafile(filename):\n",
        "  \"\"\"Reads csv file with python span list and text.\"\"\"\n",
        "  data = []\n",
        "  with open(filename) as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    count = 0\n",
        "    for row in reader:\n",
        "      fixed = fix_spans(\n",
        "          ast.literal_eval(row['spans']), row['text'])\n",
        "      data.append((fixed, row['text']))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zHDIqAMl7hp"
      },
      "source": [
        "# Read training data\r\n",
        "print('loading training data')\r\n",
        "train = read_datafile('train.csv')\r\n",
        "\r\n",
        "# Read trial data for test.\r\n",
        "print('loading test data')\r\n",
        "test = read_datafile('validation.csv')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1FjBz_pl0eX",
        "outputId": "a1a51312-be2e-4e29-9d1a-c5c17b248a43"
      },
      "source": [
        "# def main():\n",
        "\"\"\"Train and eval a spacy named entity tagger for toxic spans.\"\"\"\n",
        "\n",
        "# Convert training data to Spacy Entities\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print('preparing training data')\n",
        "training_data = []\n",
        "for n, (spans, text) in enumerate(train):\n",
        "  doc = nlp(text)\n",
        "  ents = spans_to_ents(doc, set(spans), 'TOXIC')\n",
        "  training_data.append((doc.text, {'entities': ents}))\n",
        "\n",
        "toxic_tagging = spacy.blank('en')\n",
        "toxic_tagging.vocab.strings.add('TOXIC')\n",
        "ner = nlp.create_pipe(\"ner\")\n",
        "toxic_tagging.add_pipe(ner, last=True)\n",
        "ner.add_label('TOXIC')\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [\n",
        "    pipe for pipe in toxic_tagging.pipe_names\n",
        "    if pipe not in pipe_exceptions]\n",
        "\n",
        "print('training')\n",
        "with toxic_tagging.disable_pipes(*unaffected_pipes):\n",
        "  toxic_tagging.begin_training()\n",
        "  for iteration in range(30):\n",
        "    random.shuffle(training_data)\n",
        "    losses = {}\n",
        "    batches = spacy.util.minibatch(\n",
        "        training_data, size=spacy.util.compounding(\n",
        "            4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "      texts, annotations = zip(*batch)\n",
        "      toxic_tagging.update(texts, annotations, drop=0.5, losses=losses)\n",
        "    print(\"Losses\", losses)\n",
        "\n",
        "# Score on validation data.\n",
        "    print('evaluation')\n",
        "    scores = []\n",
        "    for spans, text in test:\n",
        "        pred_spans = []\n",
        "        doc = toxic_tagging(text)\n",
        "        for ent in doc.ents:\n",
        "            pred_spans.extend(range(ent.start_char, ent.start_char + len(ent.text)))\n",
        "        score = f1(pred_spans, spans)\n",
        "        scores.append(score)\n",
        "    print('avg F1 %g' % statistics.mean(scores))\n",
        "    if(accu<statistics.mean(scores)):\n",
        "        accu=statistics.mean(scores)\n",
        "        toxic_tagging.to_disk(\"./drive/My Drive/best\")\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading training data\n",
            "loading test data\n",
            "preparing training data\n",
            "training\n",
            "Losses {'ner': 25542.330320830177}\n",
            "evaluation\n",
            "avg F1 0.561472\n",
            "Losses {'ner': 21688.396416402713}\n",
            "evaluation\n",
            "avg F1 0.487952\n",
            "Losses {'ner': 21494.040469275318}\n",
            "evaluation\n",
            "avg F1 0.540415\n",
            "Losses {'ner': 20818.088185393455}\n",
            "evaluation\n",
            "avg F1 0.595986\n",
            "Losses {'ner': 19128.068120488657}\n",
            "evaluation\n",
            "avg F1 0.544117\n",
            "Losses {'ner': 19222.784765028082}\n",
            "evaluation\n",
            "avg F1 0.557138\n",
            "Losses {'ner': 18748.401597556338}\n",
            "evaluation\n",
            "avg F1 0.587888\n",
            "Losses {'ner': 17869.124502382554}\n",
            "evaluation\n",
            "avg F1 0.586877\n",
            "Losses {'ner': 17297.707755795396}\n",
            "evaluation\n",
            "avg F1 0.550164\n",
            "Losses {'ner': 17737.55657736829}\n",
            "evaluation\n",
            "avg F1 0.601662\n",
            "Losses {'ner': 16987.667335124912}\n",
            "evaluation\n",
            "avg F1 0.562182\n",
            "Losses {'ner': 16495.107530350273}\n",
            "evaluation\n",
            "avg F1 0.570561\n",
            "Losses {'ner': 15976.735554215864}\n",
            "evaluation\n",
            "avg F1 0.599318\n",
            "Losses {'ner': 16003.423004134667}\n",
            "evaluation\n",
            "avg F1 0.565677\n",
            "Losses {'ner': 15314.599330999636}\n",
            "evaluation\n",
            "avg F1 0.557737\n",
            "Losses {'ner': 15268.023387987552}\n",
            "evaluation\n",
            "avg F1 0.565597\n",
            "Losses {'ner': 16001.788344180384}\n",
            "evaluation\n",
            "avg F1 0.581527\n",
            "Losses {'ner': 15566.629918570281}\n",
            "evaluation\n",
            "avg F1 0.544326\n",
            "Losses {'ner': 15161.596397085781}\n",
            "evaluation\n",
            "avg F1 0.618362\n",
            "Losses {'ner': 14386.353922474433}\n",
            "evaluation\n",
            "avg F1 0.591297\n",
            "Losses {'ner': 14792.346917004517}\n",
            "evaluation\n",
            "avg F1 0.589389\n",
            "Losses {'ner': 14955.719708023798}\n",
            "evaluation\n",
            "avg F1 0.591664\n",
            "Losses {'ner': 14049.911076208327}\n",
            "evaluation\n",
            "avg F1 0.57656\n",
            "Losses {'ner': 14567.933513089076}\n",
            "evaluation\n",
            "avg F1 0.612545\n",
            "Losses {'ner': 14792.218415176523}\n",
            "evaluation\n",
            "avg F1 0.591656\n",
            "Losses {'ner': 14394.631419177436}\n",
            "evaluation\n",
            "avg F1 0.579484\n",
            "Losses {'ner': 13951.309162106783}\n",
            "evaluation\n",
            "avg F1 0.592414\n",
            "Losses {'ner': 13898.952495890764}\n",
            "evaluation\n",
            "avg F1 0.580224\n",
            "Losses {'ner': 14388.754373834114}\n",
            "evaluation\n",
            "avg F1 0.581145\n",
            "Losses {'ner': 13936.690790044664}\n",
            "evaluation\n",
            "avg F1 0.584654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXpZtWBs2Rh4"
      },
      "source": [
        "# toxic_tagging.to_disk(\"besti\")\r\n",
        "test = read_datafile('trial.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBa07uJiOni7"
      },
      "source": [
        "from spacy.lang.en import Language\r\n",
        "nlpi = spacy.load(\"./drive/My Drive/best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOCTUsRm2B9U",
        "outputId": "e1f31d86-cc53-4f4f-cb12-b6930b8b51d1"
      },
      "source": [
        "print('evaluation')\r\n",
        "scores = []\r\n",
        "for spans, text in test:\r\n",
        "    pred_spans = []\r\n",
        "    doc = nlpi(text)\r\n",
        "    for ent in doc.ents:\r\n",
        "        pred_spans.extend(range(ent.start_char, ent.start_char + len(ent.text)))\r\n",
        "    score = f1(pred_spans, spans)\r\n",
        "    scores.append(score)\r\n",
        "print('avg F1 %g' % statistics.mean(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluation\n",
            "avg F1 0.663341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xewKh4sek7ih"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ7rnaJcLckn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMWZxb0lO8VZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
